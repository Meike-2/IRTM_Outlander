{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Load book\n",
    "### Note this has to be ran per book"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-14T11:46:54.538114Z",
     "end_time": "2023-05-14T11:46:54.545077Z"
    }
   },
   "outputs": [],
   "source": [
    "books = ['Outlander', 'Dragonfly in Amber', 'Voyager', 'Drums of Autumn', 'The Fiery Cross', 'A Breath of Snow and Ashes',\n",
    "             'An Echo in the Bone', 'Written in My Own Heartâ€™s Blood']\n",
    "extras = ['Other Books by this Author', 'About the Author']\n",
    "bookstarts = [50, 17287, 37378, 61857, 89432, 119494, 152540, 177800, 202059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f = open(\"data.txt\", \"r\", encoding=\"utf8\")\n",
    "booknum = 1\n",
    "\n",
    "\n",
    "book = ''\n",
    "i = 0\n",
    "for line in f:\n",
    "    if i < bookstarts[booknum] and i > bookstarts[booknum-1]:\n",
    "        book = book + line\n",
    "    i = i + 1\n",
    "print(len(book))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:21:31.083834Z",
     "end_time": "2023-05-14T13:21:31.739277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from flair.data import Sentence, Token\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger_ner = SequenceTagger.load(\"flair/ner-english-fast\")\n",
    "tagger_pos = SequenceTagger.load(\"flair/pos-english\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T15:56:23.820922Z",
     "end_time": "2023-05-14T15:56:37.833159Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tagging and splitting code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ner_tag(text):\n",
    "    sentence_ner = Sentence(text)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger_ner.predict(sentence_ner)\n",
    "    return sentence_ner"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T11:47:56.649400Z",
     "end_time": "2023-05-14T11:47:56.653387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def pos_tag(text):\n",
    "   sentence_pos = Sentence(text.split(' '), use_tokenizer=True)\n",
    "\n",
    "   # predict NER tags\n",
    "   tagger_pos.predict(sentence_pos)\n",
    "   return sentence_pos"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T16:11:52.140533Z",
     "end_time": "2023-05-14T16:11:52.154160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_person_before(sentence_ner, token):\n",
    "    # If text is 'it' then don't change it\n",
    "    if token.text=='it':\n",
    "        return token.text\n",
    "    # If text is 'I' or 'me' then input Claire as it's first person\n",
    "    if token.text=='I' or token.text=='me':\n",
    "        return 'Claire'\n",
    "    prev_person = ''\n",
    "    # Else input the previous name\n",
    "    for entity in sentence_ner.get_spans('ner'):\n",
    "        if entity.tag == 'PERSON' and entity.start_position<token.start_position:\n",
    "                prev_person = entity.text\n",
    "        elif entity.start_position>token.start_position:\n",
    "            break\n",
    "    return prev_person"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T11:47:56.678320Z",
     "end_time": "2023-05-14T11:47:56.729185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "def split_into_chapters(booknum, text):\n",
    "    if booknum <7:\n",
    "        return re.split(r'(\\\\n[0-9]+\\\\n\\\\n)', text)\n",
    "    else:\n",
    "        # needs refinement\n",
    "        return re.split(r\"\\n(?=[A-Z ]+\\n)\", text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:29:01.049371Z",
     "end_time": "2023-05-14T13:29:01.062339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text):\n",
    "    splitted = re.split('(\\*\\*\\*)', text)\n",
    "    return [x for x in splitted if x != '']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:13:57.788631Z",
     "end_time": "2023-05-14T13:13:57.814013Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Replace and write to file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "chapters = re.split(r'\\n[0-9]+\\n\\n', book)\n",
    "i = 0\n",
    "for chapter in chapters:\n",
    "    paragraphs = split_into_paragraphs(chapter)\n",
    "    for paragraph in paragraphs:\n",
    "            print('Chapter ', i)\n",
    "            print('Pos tagging....')\n",
    "            sentence_pos = pos_tag(paragraph)\n",
    "            print('Ner tagging....')\n",
    "            sentence_ner = ner_tag(paragraph)\n",
    "            no_pronouns_book = Sentence(paragraph.split(' '), use_tokenizer=True)\n",
    "            print('Replacing pronouns....')\n",
    "            i=0\n",
    "            for token in sentence_pos:\n",
    "                if token.tag == 'PRP':\n",
    "                    name = get_person_before(sentence_ner, token)\n",
    "                    new_token = Token(name)\n",
    "                    no_pronouns_book.tokens[token.idx-1] = new_token\n",
    "            with open('data_no_pronouns.txt', 'a') as f:\n",
    "                f.write(no_pronouns_book.to_original_text())\n",
    "    i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T16:41:06.932032Z",
     "end_time": "2023-05-14T16:41:06.943033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c = re.split(r'\\n[0-9]+\\n\\n', book)\n",
    "c[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
