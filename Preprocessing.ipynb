{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-14T11:46:54.538114Z",
     "end_time": "2023-05-14T11:46:54.545077Z"
    }
   },
   "outputs": [],
   "source": [
    "books = ['Outlander', 'Dragonfly in Amber', 'Voyager', 'Drums of Autumn', 'The Fiery Cross', 'A Breath of Snow and Ashes',\n",
    "             'An Echo in the Bone', 'Written in My Own Heart’s Blood']\n",
    "extras = ['Other Books by this Author', 'About the Author']\n",
    "bookstarts = [50, 17287, 37378, 61857, 89432, 119494, 152540, 177800, 202059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1675799\n"
     ]
    }
   ],
   "source": [
    "f = open(\"data.txt\", \"r\", encoding=\"utf8\")\n",
    "booknum = 1\n",
    "\n",
    "\n",
    "book = ''\n",
    "i = 0\n",
    "for line in f:\n",
    "    if i < bookstarts[booknum] and i > bookstarts[booknum-1]:\n",
    "        book = book + line\n",
    "    i = i + 1\n",
    "print(len(book))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:21:31.083834Z",
     "end_time": "2023-05-14T13:21:31.739277Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflair\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sentence, Token\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflair\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SequenceTagger\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# load tagger\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\__init__.py:28\u001B[0m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# global variable: arrow symbol\u001B[39;00m\n\u001B[0;32m     26\u001B[0m _arrow \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m → \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E402 import after setting device\u001B[39;00m\n\u001B[0;32m     29\u001B[0m     data,\n\u001B[0;32m     30\u001B[0m     models,\n\u001B[0;32m     31\u001B[0m     nn,\n\u001B[0;32m     32\u001B[0m     trainers,\n\u001B[0;32m     33\u001B[0m     visual,\n\u001B[0;32m     34\u001B[0m )\n\u001B[0;32m     36\u001B[0m logging\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mdictConfig(\n\u001B[0;32m     37\u001B[0m     {\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mversion\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     50\u001B[0m     }\n\u001B[0;32m     51\u001B[0m )\n\u001B[0;32m     53\u001B[0m logger \u001B[38;5;241m=\u001B[39m logging\u001B[38;5;241m.\u001B[39mgetLogger(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mflair\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\trainers\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlanguage_model_trainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LanguageModelTrainer, TextCorpus\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtrainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ModelTrainer\n\u001B[0;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModelTrainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLanguageModelTrainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextCorpus\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\trainers\\trainer.py:17\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01moptim\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msgd\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SGD\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdataset\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConcatDataset\n\u001B[1;32m---> 17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformer_smaller_training_vocab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reduce_train_vocab\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflair\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01membeddings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Embeddings, StackedEmbeddings, TransformerEmbeddings\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mflair\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FewshotClassifier\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformer_smaller_training_vocab\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformer_smaller_training_vocab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontextual_reduce\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m reduce_train_vocab\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformer_smaller_training_vocab\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_texts_from_dataset\n\u001B[0;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreduce_train_vocab\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget_texts_from_dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\transformer_smaller_training_vocab\\utils.py:3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Union, Tuple, Iterator, cast, Dict, List\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dataset, DatasetDict\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtokenization_utils_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TextInput, PreTokenizedInput, TextInputPair, PreTokenizedInputPair\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_texts_from_dataset\u001B[39m(\n\u001B[0;32m      8\u001B[0m     dataset: Union[Dataset, DatasetDict], key: Union[\u001B[38;5;28mstr\u001B[39m, Tuple[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]]\n\u001B[0;32m      9\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Iterator[Union[TextInput, PreTokenizedInput, TextInputPair, PreTokenizedInputPair]]:\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\__init__.py:24\u001B[0m\n\u001B[0;32m     20\u001B[0m __version__ \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m2.12.0\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mplatform\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m version\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m version\u001B[38;5;241m.\u001B[39mparse(platform\u001B[38;5;241m.\u001B[39mpython_version()) \u001B[38;5;241m<\u001B[39m version\u001B[38;5;241m.\u001B[39mparse(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m3.7\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\pyarrow\\__init__.py:63\u001B[0m\n\u001B[0;32m     61\u001B[0m _gc_enabled \u001B[38;5;241m=\u001B[39m _gc\u001B[38;5;241m.\u001B[39misenabled()\n\u001B[0;32m     62\u001B[0m _gc\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[1;32m---> 63\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpyarrow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlib\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_lib\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _gc_enabled:\n\u001B[0;32m     65\u001B[0m     _gc\u001B[38;5;241m.\u001B[39menable()\n",
      "\u001B[1;31mImportError\u001B[0m: DLL load failed while importing lib: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence, Token\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "# load tagger\n",
    "tagger_ner = SequenceTagger.load(\"flair/ner-english-fast\")\n",
    "tagger_pos = SequenceTagger.load(\"flair/pos-english\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T15:56:23.820922Z",
     "end_time": "2023-05-14T15:56:37.833159Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def ner_tag(text):\n",
    "    sentence_ner = Sentence(text)\n",
    "\n",
    "    # predict NER tags\n",
    "    tagger_ner.predict(sentence_ner)\n",
    "    return sentence_ner"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T11:47:56.649400Z",
     "end_time": "2023-05-14T11:47:56.653387Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def pos_tag(text):\n",
    "   sentence_pos = Sentence(text.split(' '), use_tokenizer=True)\n",
    "\n",
    "   # predict NER tags\n",
    "   tagger_pos.predict(sentence_pos)\n",
    "   return sentence_pos"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T16:11:52.140533Z",
     "end_time": "2023-05-14T16:11:52.154160Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_person_before(sentence_ner, token):\n",
    "    if token.text=='it':\n",
    "        return token.text\n",
    "    if token.text=='I' or token.text=='me':\n",
    "        return 'Claire'\n",
    "    prev_person = ''\n",
    "    for entity in sentence_ner.get_spans('ner'):\n",
    "        if entity.tag == 'PERSON' and entity.start_position<token.start_position:\n",
    "                prev_person = entity.text\n",
    "        elif entity.start_position>token.start_position:\n",
    "            break\n",
    "    return prev_person"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T11:47:56.678320Z",
     "end_time": "2023-05-14T11:47:56.729185Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import re\n",
    "def split_into_chapters(booknum, text):\n",
    "    if booknum <7:\n",
    "        return re.split(r'(\\\\n[0-9]+\\\\n\\\\n)', book)\n",
    "    else:\n",
    "        # needs refinement\n",
    "        return re.split(r\"\\n(?=[A-Z ]+\\n)\", string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:29:01.049371Z",
     "end_time": "2023-05-14T13:29:01.062339Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text):\n",
    "    splitted = re.split('(\\*\\*\\*)', text)\n",
    "    return [x for x in splitted if x != '']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:13:57.788631Z",
     "end_time": "2023-05-14T13:13:57.814013Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter  0\n",
      "Pos tagging....\n",
      "Ner tagging....\n",
      "Replacing pronouns....\n",
      "Chapter  1\n",
      "Pos tagging....\n",
      "Ner tagging....\n",
      "Replacing pronouns....\n",
      "Chapter  1\n",
      "Pos tagging....\n",
      "Ner tagging....\n",
      "Replacing pronouns....\n",
      "Chapter  1\n",
      "Pos tagging....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[105], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mChapter \u001B[39m\u001B[38;5;124m'\u001B[39m, i)\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPos tagging....\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m sentence_pos \u001B[38;5;241m=\u001B[39m \u001B[43mpos_tag\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparagraph\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNer tagging....\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     10\u001B[0m sentence_ner \u001B[38;5;241m=\u001B[39m ner_tag(paragraph)\n",
      "Cell \u001B[1;32mIn[87], line 5\u001B[0m, in \u001B[0;36mpos_tag\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      2\u001B[0m sentence_pos \u001B[38;5;241m=\u001B[39m Sentence(text\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m), use_tokenizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# predict NER tags\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m \u001B[43mtagger_pos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence_pos\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sentence_pos\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:483\u001B[0m, in \u001B[0;36mSequenceTagger.predict\u001B[1;34m(self, sentences, mini_batch_size, return_probabilities_for_all_classes, verbose, label_name, return_loss, embedding_storage_mode, force_token_predictions)\u001B[0m\n\u001B[0;32m    480\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    482\u001B[0m \u001B[38;5;66;03m# get features from forward propagation\u001B[39;00m\n\u001B[1;32m--> 483\u001B[0m sentence_tensor, lengths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_tensors\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    484\u001B[0m features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(sentence_tensor, lengths)\n\u001B[0;32m    486\u001B[0m \u001B[38;5;66;03m# remove previously predicted labels of this type\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\models\\sequence_tagger_model.py:287\u001B[0m, in \u001B[0;36mSequenceTagger._prepare_tensors\u001B[1;34m(self, data_points)\u001B[0m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    286\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m data_points\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membeddings\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    289\u001B[0m \u001B[38;5;66;03m# make a zero-padded tensor for the whole sentence\u001B[39;00m\n\u001B[0;32m    290\u001B[0m lengths, sentence_tensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_padded_tensor_for_batch(sentences)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\embeddings\\token.py:101\u001B[0m, in \u001B[0;36mStackedEmbeddings.embed\u001B[1;34m(self, sentences, static_embeddings)\u001B[0m\n\u001B[0;32m     98\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m [sentences]\n\u001B[0;32m    100\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m embedding \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings:\n\u001B[1;32m--> 101\u001B[0m     \u001B[43membedding\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentences\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\embeddings\\base.py:49\u001B[0m, in \u001B[0;36mEmbeddings.embed\u001B[1;34m(self, data_points)\u001B[0m\n\u001B[0;32m     46\u001B[0m     data_points \u001B[38;5;241m=\u001B[39m [data_points]\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_everything_embedded(data_points):\n\u001B[1;32m---> 49\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_add_embeddings_internal\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_points\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data_points\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\embeddings\\token.py:819\u001B[0m, in \u001B[0;36mFlairEmbeddings._add_embeddings_internal\u001B[1;34m(self, sentences)\u001B[0m\n\u001B[0;32m    816\u001B[0m end_marker \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    818\u001B[0m \u001B[38;5;66;03m# get hidden states from language model\u001B[39;00m\n\u001B[1;32m--> 819\u001B[0m all_hidden_states_in_lm \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_representation\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    820\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtext_sentences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_marker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mend_marker\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchars_per_chunk\u001B[49m\n\u001B[0;32m    821\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    823\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfine_tune:\n\u001B[0;32m    824\u001B[0m     all_hidden_states_in_lm \u001B[38;5;241m=\u001B[39m all_hidden_states_in_lm\u001B[38;5;241m.\u001B[39mdetach()\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\models\\language_model.py:161\u001B[0m, in \u001B[0;36mLanguageModel.get_representation\u001B[1;34m(self, strings, start_marker, end_marker, chars_per_chunk)\u001B[0m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m batches:\n\u001B[0;32m    160\u001B[0m     batch \u001B[38;5;241m=\u001B[39m batch\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m--> 161\u001B[0m     rnn_output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    162\u001B[0m     output_parts\u001B[38;5;241m.\u001B[39mappend(rnn_output)\n\u001B[0;32m    164\u001B[0m \u001B[38;5;66;03m# concatenate all chunks to make final output\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\flair\\models\\language_model.py:89\u001B[0m, in \u001B[0;36mLanguageModel.forward\u001B[1;34m(self, input, hidden, ordered_sequence_lengths, decode)\u001B[0m\n\u001B[0;32m     87\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m (h,)\n\u001B[0;32m     88\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 89\u001B[0m     output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrnn\u001B[49m\u001B[43m(\u001B[49m\u001B[43memb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     92\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mproj(output)\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1186\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1187\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1191\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1192\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:774\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcheck_forward_args(\u001B[38;5;28minput\u001B[39m, hx, batch_sizes)\n\u001B[0;32m    773\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m batch_sizes \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 774\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    775\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_first\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    776\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    777\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\u001B[38;5;28minput\u001B[39m, batch_sizes, hx, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flat_weights, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias,\n\u001B[0;32m    778\u001B[0m                       \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbidirectional)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "chapters = re.split(r'\\n[0-9]+\\n\\n', book)\n",
    "i = 0\n",
    "for chapter in chapters:\n",
    "    paragraphs = split_into_paragraphs(chapter)\n",
    "    for paragraph in paragraphs:\n",
    "            print('Chapter ', i)\n",
    "            print('Pos tagging....')\n",
    "            sentence_pos = pos_tag(paragraph)\n",
    "            print('Ner tagging....')\n",
    "            sentence_ner = ner_tag(paragraph)\n",
    "            no_pronouns_book = Sentence(paragraph.split(' '), use_tokenizer=True)\n",
    "            print('Replacing pronouns....')\n",
    "            i=0\n",
    "            for token in sentence_pos:\n",
    "                if token.tag == 'PRP':\n",
    "                    name = get_person_before(sentence_ner, token)\n",
    "                    new_token = Token(name)\n",
    "                    no_pronouns_book.tokens[token.idx-1] = new_token\n",
    "            with open('data_no_pronouns.txt', 'a') as f:\n",
    "                f.write(no_pronouns_book.to_original_text())\n",
    "    i+=1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T16:41:06.932032Z",
     "end_time": "2023-05-14T16:41:06.943033Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown \n",
      " fox jumps over the lazy dog.\n",
      "Token[0]: \"The\" → DT (1.0)\n",
      "Token[1]: \"quick\" → JJ (0.9988)\n",
      "Token[2]: \"brown\" → JJ (0.9641)\n",
      "Token[-2]: \"test\"\n",
      "Token[3]: \"\n",
      "\" → HYPH (0.2374)\n",
      "Token[4]: \"fox\" → NNP (0.6537)\n",
      "Token[5]: \"jumps\" → VBZ (1.0)\n",
      "Token[6]: \"over\" → IN (0.89)\n",
      "Token[7]: \"the\" → DT (1.0)\n",
      "Token[8]: \"lazy\" → JJ (0.9982)\n",
      "Token[9]: \"dog.\" → NN (0.9999)\n",
      "The quick test \n",
      " fox jumps over the lazy dog.\n",
      "Token[0]: \"The\"\n",
      "Token[1]: \"quick\"\n",
      "Token[-2]: \"test\"\n",
      "Token[3]: \"\n",
      "\"\n",
      "Token[4]: \"fox\"\n",
      "Token[5]: \"jumps\"\n",
      "Token[6]: \"over\"\n",
      "Token[7]: \"the\"\n",
      "Token[8]: \"lazy\"\n",
      "Token[9]: \"dog.\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence = Sentence('The quick brown \\n fox jumps over the lazy dog.'.split(' '), use_tokenizer=True)\n",
    "\n",
    "# get the token to replace\n",
    "token_to_replace = sentence[1]\n",
    "\n",
    "# create a new token with the desired text\n",
    "new_token_text = 'black'\n",
    "new_token = Token(new_token_text)\n",
    "\n",
    "# replace the old token with the new one\n",
    "sentence.tokens[token_to_replace.idx-1] = new_token\n",
    "\n",
    "# print the modified sentence\n",
    "print(sentence.to_tokenized_string())\n",
    "\n",
    "\n",
    "sentence_pos = pos_tag('The quick brown \\n fox jumps over the lazy dog.')\n",
    "\n",
    "i=0\n",
    "for token in sentence_pos:\n",
    "    if i==1:\n",
    "        new_token = Token('test')\n",
    "        sentence.tokens[token.idx-1] = new_token\n",
    "        print(new_token)\n",
    "    i+=1\n",
    "\n",
    "print(sentence.to_tokenized_string())\n",
    "\n",
    "for token in sentence:\n",
    "    print(token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T16:16:32.589465Z",
     "end_time": "2023-05-14T16:16:33.027291Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "c = re.split(r'\\n[0-9]+\\n\\n', book)\n",
    "c[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "'The quick brown \\n fox jumps over the lazy dog.'.split(' ')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentence = Sentence([\"I\", \"\\t\", \"ich\", \"\\n\", \"you\", \"\\t\", \"du\", \"\\n\"], use_tokenizer=True)\n",
    "\n",
    "# Alternatively: sentence: Sentence = Sentence(\"I \\t ich \\n you \\t du \\n\", use_tokenizer=False)\n",
    "\n",
    "# print sentence and each token\n",
    "print(sentence)\n",
    "for token in sentence:\n",
    "    print(token)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "['Hello world!',\n 'THIS LINE IS ALL CAPS\\nPython is fun.',\n 'ANOTHER ALL CAPS LINE\\n']"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Hello world!\\nTHIS LINE IS ALL CAPS\\nPython is fun.\\nANOTHER ALL CAPS LINE\\n\"\n",
    "result = re.split(r\"\\n(?=[A-Z ]+\\n)\", string)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-15T02:20:43.240622Z",
     "end_time": "2023-05-15T02:20:43.262306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
