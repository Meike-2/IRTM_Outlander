{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:20:03.691844Z",
     "end_time": "2023-05-14T12:20:16.005914Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\simiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\simiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\simiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\simiv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import defaultdict\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk.chunk import ne_chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Bookwise preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'3.7'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:08:01.170983Z",
     "end_time": "2023-05-14T12:08:01.249769Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "books = ['Outlander', 'Dragonfly in Amber', 'Voyager', 'Drums of Autumn', 'The Fiery Cross', 'A Breath of Snow and Ashes',\n",
    "         'An Echo in the Bone', 'Written in My Own Heart’s Blood']\n",
    "extras = ['Other Books by this Author', 'About the Author']\n",
    "bookstarts = [50, 17287, 37378, 61857, 89432, 119494, 152540, 177800, 202059]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:08:01.187937Z",
     "end_time": "2023-05-14T12:08:01.250774Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "f = open(\"data.txt\", \"r\", encoding=\"utf8\")\n",
    "booknum = 1\n",
    "\n",
    "\n",
    "book = ''\n",
    "i = 0\n",
    "for line in f:\n",
    "    if i < bookstarts[booknum] and i > bookstarts[booknum-1]:\n",
    "        book = book + line\n",
    "    i = i + 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:08:04.081202Z",
     "end_time": "2023-05-14T12:08:05.807585Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# we want to segment the data into chapters as we want to see the topic per chapter. But how do we do that?\n",
    "# the author devided the books into parts, this is how we can see when hte first chapter begins, then we search for a line\n",
    "# that is written in all caps, after this the chapter starts: example: \\n\\n\\n\\n\\n\\n1\\n\\nA NEW BEGINNING\\n\\n\n",
    "#\n",
    "# book 1 has 41, 49, 63, 71, 111, 124, 103, 145\n",
    "# found is 41, 49, 63, 71, 111, 124, 0, 0\n",
    "def split_into_chapters(booknum, text):\n",
    "    if booknum <7:\n",
    "        return re.split(r'\\n[0-9]+\\n\\n', book)\n",
    "    else:\n",
    "        # needs refinement\n",
    "        return re.split(r'[A-Z ]+\\n\\n', book)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:08:06.043952Z",
     "end_time": "2023-05-14T12:08:06.100807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text):\n",
    "    splitted = re.split('(\\*\\*\\*)', text)\n",
    "    return [x for x in splitted if x != '']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:17:32.055368Z",
     "end_time": "2023-05-14T13:17:32.065343Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'Dragonfly in Amber\\n\\nVoyager\\n\\nDrums of Autumn\\n\\nThe Fiery Cross\\n\\nA Breath of Snow and Ashes\\n\\nAn Echo in the Bone\\n\\nWritten in My Own Heart’s Blood\\n\\nOther Books by this Author\\n\\nAbout the Author\\n\\n\\n\\n\\n\\nOUTLANDER\\n\\nA Delta Book\\n\\nPUBLISHING HISTORY\\n\\nDelacorte Press hardcover edition published 1991\\n\\nDelta trade paperback edition/July 2001\\n\\nPublished by\\n\\nBantam Dell\\n\\nA Division of Random House, Inc.\\n\\nNew York, New York\\n\\nAll rights reserved\\n\\nCopyright © 1991 by Diana Gabaldon\\n\\nTitle page art copyright © 1999 by Barbara Schnell\\n\\nLibrary of Congress Catalog Card Number: 90-019122\\n\\nNo part of this book may be reproduced or transmitted in any form or by any means, electronic or mechanical, including photocopying, recording, or by any information storage and retrieval system, without the written permission of the publisher, except where permitted by law.\\n\\nDelta is a registered trademark of Random House, Inc., and the colophon is a trademark of Random House, Inc.\\n\\nPlease visit our website at www.bantamdell.com\\n\\neISBN: 978-0-440-33516-0\\n\\nv3.0_r3\\n\\n\\n\\n\\n\\nContents\\n\\nMaster - Table of Contents\\n\\n\\n\\n\\n\\nOutlander\\n\\nTitle page\\n\\nCopyright\\n\\nEpigraph\\n\\nPart One\\n\\nChapter 1\\n\\nChapter 2\\n\\nChapter 3\\n\\nChapter 4\\n\\nChapter 5\\n\\nPart Two\\n\\nChapter 6\\n\\nChapter 7\\n\\nChapter 8\\n\\nChapter 9\\n\\nChapter 10\\n\\nPart Three\\n\\nChapter 11\\n\\nChapter 12\\n\\nChapter 13\\n\\nChapter 14\\n\\nChapter 15\\n\\nChapter 16\\n\\nChapter 17\\n\\nChapter 18\\n\\nChapter 19\\n\\nChapter 20\\n\\nChapter 21\\n\\nChapter 22\\n\\nChapter 23\\n\\nPart Four\\n\\nChapter 24\\n\\nChapter 25\\n\\nPart Five\\n\\nChapter 26\\n\\nChapter 27\\n\\nChapter 28\\n\\nChapter 29\\n\\nChapter 30\\n\\nChapter 31\\n\\nChapter 32\\n\\nChapter 33\\n\\nPart Six\\n\\nChapter 34\\n\\nPart Seven\\n\\nChapter 35\\n\\nChapter 36\\n\\nChapter 37\\n\\nChapter 38\\n\\nChapter 39\\n\\nChapter 40\\n\\nChapter 41\\n\\n\\n\\n\\n\\nDedication\\n\\nAcknowledgments\\n\\nAn Intervew with Diana Gabaldon\\n\\nReader’s Guide\\n\\n\\n\\n\\n\\nPeople disappear all the time. Ask any policeman. Better yet, ask a journalist. Disappearances are bread-and-butter to journalists.\\n\\nYoung girls run away from home. Young children stray from their parents and are never seen again. Housewives reach the end of their tether and take the grocery money and a taxi to the station. International financiers change their names and vanish into the smoke of imported cigars.\\n\\nMany of the lost will be found, eventually, dead or alive. Disappearances, after all, have explanations.\\n\\nUsually.\\n\\n\\n\\n\\n\\nPART ONE\\n\\n\\n\\n\\n\\nInverness, 1945\\n\\n\\n\\n\\n'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(split_into_paragraphs(split_into_chapters(1, book)[4]))\n",
    "sentences = split_into_paragraphs(split_into_chapters(1, book)[0])\n",
    "sentences[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T13:23:57.263479Z",
     "end_time": "2023-05-14T13:23:57.286419Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Linguistic Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "per_paragraph = True\n",
    "\n",
    "tokenizer = RegexpTokenizer(r\"[a-zA-Z’]+\")\n",
    "chapters = split_into_chapters(booknum, book)\n",
    "tchapters = []\n",
    "for chapter in chapters:\n",
    "    tparagraphs = []\n",
    "    if per_paragraph:\n",
    "        paragraphs = split_into_paragraphs(chapter)\n",
    "        for paragraph in paragraphs:\n",
    "            tparagraphs.append(tokenizer.tokenize(paragraph))\n",
    "        tchapters.append(tparagraphs)\n",
    "    else:\n",
    "        tchapters.append(tokenizer.tokenize(chapter))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:08:44.180971Z",
     "end_time": "2023-05-14T12:08:44.355507Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dragonfly\n",
      "in\n",
      "Amber\n",
      "Voyager\n",
      "Drums\n",
      "of\n",
      "Autumn\n",
      "The\n",
      "Fiery\n",
      "Cross\n",
      "A\n",
      "Breath\n",
      "of\n",
      "Snow\n",
      "and\n",
      "Ashes\n",
      "An\n",
      "Echo\n",
      "in\n",
      "the\n",
      "Bone\n",
      "Written\n",
      "in\n",
      "My\n",
      "Own\n",
      "Heart’s\n",
      "Blood\n",
      "Other\n",
      "Books\n",
      "by\n",
      "this\n",
      "Author\n",
      "About\n",
      "the\n",
      "Author\n",
      "OUTLANDER\n",
      "A\n",
      "Delta\n",
      "Book\n",
      "PUBLISHING\n",
      "HISTORY\n",
      "Delacorte\n",
      "Press\n",
      "hardcover\n",
      "edition\n",
      "published\n",
      "Delta\n",
      "trade\n",
      "paperback\n",
      "edition\n",
      "July\n",
      "Published\n",
      "by\n",
      "Bantam\n",
      "Dell\n",
      "A\n",
      "Division\n",
      "of\n",
      "Random\n",
      "House\n",
      "Inc\n",
      "New\n",
      "York\n",
      "New\n",
      "York\n",
      "All\n",
      "right\n",
      "reserved\n",
      "Copyright\n",
      "by\n",
      "Diana\n",
      "Gabaldon\n",
      "Title\n",
      "page\n",
      "art\n",
      "copyright\n",
      "by\n",
      "Barbara\n",
      "Schnell\n",
      "Library\n",
      "of\n",
      "Congress\n",
      "Catalog\n",
      "Card\n",
      "Number\n",
      "No\n",
      "part\n",
      "of\n",
      "this\n",
      "book\n",
      "may\n",
      "be\n",
      "reproduced\n",
      "or\n",
      "transmitted\n",
      "in\n",
      "any\n",
      "form\n",
      "or\n",
      "by\n",
      "any\n",
      "mean\n",
      "electronic\n",
      "or\n",
      "mechanical\n",
      "including\n",
      "photocopying\n",
      "recording\n",
      "or\n",
      "by\n",
      "any\n",
      "information\n",
      "storage\n",
      "and\n",
      "retrieval\n",
      "system\n",
      "without\n",
      "the\n",
      "written\n",
      "permission\n",
      "of\n",
      "the\n",
      "publisher\n",
      "except\n",
      "where\n",
      "permitted\n",
      "by\n",
      "law\n",
      "Delta\n",
      "is\n",
      "a\n",
      "registered\n",
      "trademark\n",
      "of\n",
      "Random\n",
      "House\n",
      "Inc\n",
      "and\n",
      "the\n",
      "colophon\n",
      "is\n",
      "a\n",
      "trademark\n",
      "of\n",
      "Random\n",
      "House\n",
      "Inc\n",
      "Please\n",
      "visit\n",
      "our\n",
      "website\n",
      "at\n",
      "www\n",
      "bantamdell\n",
      "com\n",
      "eISBN\n",
      "v\n",
      "r\n",
      "Contents\n",
      "Master\n",
      "Table\n",
      "of\n",
      "Contents\n",
      "Outlander\n",
      "Title\n",
      "page\n",
      "Copyright\n",
      "Epigraph\n",
      "Part\n",
      "One\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Part\n",
      "Two\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Part\n",
      "Three\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Part\n",
      "Four\n",
      "Chapter\n",
      "Chapter\n",
      "Part\n",
      "Five\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Part\n",
      "Six\n",
      "Chapter\n",
      "Part\n",
      "Seven\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Chapter\n",
      "Dedication\n",
      "Acknowledgments\n",
      "An\n",
      "Intervew\n",
      "with\n",
      "Diana\n",
      "Gabaldon\n",
      "Reader’s\n",
      "Guide\n",
      "People\n",
      "disappear\n",
      "all\n",
      "the\n",
      "time\n",
      "Ask\n",
      "any\n",
      "policeman\n",
      "Better\n",
      "yet\n",
      "ask\n",
      "a\n",
      "journalist\n",
      "Disappearances\n",
      "are\n",
      "bread\n",
      "and\n",
      "butter\n",
      "to\n",
      "journalist\n",
      "Young\n",
      "girl\n",
      "run\n",
      "away\n",
      "from\n",
      "home\n",
      "Young\n",
      "child\n",
      "stray\n",
      "from\n",
      "their\n",
      "parent\n",
      "and\n",
      "are\n",
      "never\n",
      "seen\n",
      "again\n",
      "Housewives\n",
      "reach\n",
      "the\n",
      "end\n",
      "of\n",
      "their\n",
      "tether\n",
      "and\n",
      "take\n",
      "the\n",
      "grocery\n",
      "money\n",
      "and\n",
      "a\n",
      "taxi\n",
      "to\n",
      "the\n",
      "station\n",
      "International\n",
      "financier\n",
      "change\n",
      "their\n",
      "name\n",
      "and\n",
      "vanish\n",
      "into\n",
      "the\n",
      "smoke\n",
      "of\n",
      "imported\n",
      "cigar\n",
      "Many\n",
      "of\n",
      "the\n",
      "lost\n",
      "will\n",
      "be\n",
      "found\n",
      "eventually\n",
      "dead\n",
      "or\n",
      "alive\n",
      "Disappearances\n",
      "after\n",
      "all\n",
      "have\n",
      "explanation\n",
      "Usually\n",
      "PART\n",
      "ONE\n",
      "Inverness\n"
     ]
    }
   ],
   "source": [
    "chapternum = 0\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "if not per_paragraph:\n",
    "    for word in tchapters[chapternum]:\n",
    "        print(lemmatizer.lemmatize(word))\n",
    "else:\n",
    "    for paragraph in tchapters[chapternum]:\n",
    "        for word in paragraph:\n",
    "            print(lemmatizer.lemmatize(word))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:08:50.252735Z",
     "end_time": "2023-05-14T12:08:55.269322Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-14 12:27:36,546 SequenceTagger predicts: Dictionary with 75 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-GPE, B-GPE, E-GPE, I-GPE, S-ORG, B-ORG, E-ORG, I-ORG, S-DATE, B-DATE, E-DATE, I-DATE, S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-NORP, B-NORP, E-NORP, I-NORP, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL, S-LOC, B-LOC, E-LOC, I-LOC, S-TIME, B-TIME, E-TIME, I-TIME, S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART, S-FAC\n",
      "Span[14:15]: \"Baird’s\" → PERSON (0.9999)\n",
      "Span[46:47]: \"Baird\" → PERSON (0.9999)\n",
      "Span[57:58]: \"Frank\" → PERSON (0.9971)\n",
      "Span[3:4]: \"Baird\" → PERSON (0.9828)\n",
      "Span[3:4]: \"Randall\" → PERSON (0.9983)\n",
      "Span[23:24]: \"Ye\" → PERSON (0.9835)\n",
      "Span[1:2]: \"I’ll\" → PERSON (0.7898)\n",
      "Span[5:6]: \"Baird\" → PERSON (0.9879)\n",
      "Span[17:19]: \"Frank We’ll\" → PERSON (0.877)\n",
      "Span[44:46]: \"four years\" → DATE (0.8341)\n",
      "Span[32:33]: \"Frank\" → PERSON (0.9979)\n",
      "Span[49:50]: \"Baird\" → PERSON (0.9985)\n",
      "Span[12:14]: \"Highlands Frank\" → PERSON (0.4751)\n",
      "Span[15:16]: \"Brighton\" → GPE (0.933)\n",
      "Span[29:30]: \"Frank\" → PERSON (0.9992)\n",
      "Span[39:40]: \"Oxford\" → GPE (0.5427)\n",
      "Span[44:45]: \"Scotland\" → GPE (0.9951)\n",
      "Span[60:61]: \"Britain\" → GPE (0.9964)\n",
      "Span[26:28]: \"two day\" → DATE (0.8674)\n",
      "Span[38:40]: \"seven years\" → DATE (0.8145)\n",
      "Span[82:83]: \"Scotland\" → GPE (0.9861)\n",
      "Span[7:8]: \"Frank\" → PERSON (0.9922)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9969)\n",
      "Span[6:7]: \"You’re\" → PERSON (0.9946)\n",
      "Span[18:19]: \"She’ll\" → PERSON (0.9167)\n",
      "Span[0:2]: \"Lazybones You’ll\" → PERSON (0.8386)\n",
      "Span[11:12]: \"Highlands\" → GPE (0.3011)\n",
      "Span[45:50]: \"the middle of the eighteenth\" → DATE (0.898)\n",
      "Span[53:55]: \"seventeenth century\" → DATE (0.6998)\n",
      "Span[28:31]: \"almost eight years\" → DATE (0.7745)\n",
      "Span[31:34]: \"Little Frank Jr\" → PERSON (0.691)\n",
      "Span[14:16]: \"the week\" → DATE (0.9144)\n",
      "Span[20:21]: \"Highland\" → GPE (0.3611)\n",
      "Span[11:12]: \"Baird\" → PERSON (0.9989)\n",
      "Span[18:19]: \"Aren’t\" → PERSON (0.9484)\n",
      "Span[28:30]: \"Reginald Wakefield\" → PERSON (0.7478)\n",
      "Span[32:33]: \"Willy\" → PERSON (0.9826)\n",
      "Span[34:35]: \"Walter\" → PERSON (0.8863)\n",
      "Span[3:5]: \"Jonathan Frank\" → PERSON (0.7853)\n",
      "Span[40:41]: \"Randalls\" → PERSON (0.9628)\n",
      "Span[0:2]: \"Jonathan Wolverton\" → PERSON (0.9677)\n",
      "Span[2:4]: \"Randall Wolverton\" → PERSON (0.7792)\n",
      "Span[12:13]: \"Sussex\" → GPE (0.9174)\n",
      "Span[23:25]: \"Black Jack\" → PERSON (0.5626)\n",
      "Span[52:53]: \"Frank\" → PERSON (0.9978)\n",
      "Span[5:8]: \"the mid thirties\" → DATE (0.7252)\n",
      "Span[23:25]: \"Cousin May\" → PERSON (0.8019)\n",
      "Span[63:65]: \"Jack Randall\" → PERSON (0.8859)\n",
      "Span[102:105]: \"Bonnie Prince Charlie\" → PERSON (0.7574)\n",
      "Span[35:36]: \"Sassenachs\" → PERSON (0.4531)\n",
      "Span[4:5]: \"Frank\" → PERSON (0.9986)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9951)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.978)\n",
      "Span[2:3]: \"You’ve\" → PERSON (0.9793)\n",
      "Span[7:8]: \"Claire\" → PERSON (0.7201)\n",
      "2023-05-14 12:28:16,489 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Span[34:35]: \"Baird\" → PERSON (0.9841)\n",
      "Span[32:33]: \"France\" → GPE (0.9983)\n",
      "Span[59:61]: \"Uncle Lamb\" → PERSON (0.6983)\n",
      "Span[0:4]: \"Quentin Lambert Beauchamp Q\" → PERSON (0.927)\n",
      "Span[12:13]: \"Beauchamp\" → PERSON (0.8158)\n",
      "Span[29:31]: \"Uncle Lamb\" → PERSON (0.7701)\n",
      "Span[19:20]: \"five\" → DATE (0.6763)\n",
      "Span[26:28]: \"Uncle Lamb\" → PERSON (0.7908)\n",
      "Span[0:1]: \"Ruddy\" → PERSON (0.9426)\n",
      "Span[72:74]: \"Uncle Lamb\" → PERSON (0.7236)\n",
      "Span[4:5]: \"Frank\" → PERSON (0.9991)\n",
      "Span[48:51]: \"nearly eight years\" → DATE (0.7085)\n",
      "Span[55:56]: \"Oxford\" → GPE (0.9816)\n",
      "2023-05-14 12:28:32,012 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Span[2:3]: \"Frank\" → PERSON (0.9963)\n",
      "Span[14:15]: \"Claire\" → PERSON (0.8721)\n",
      "Span[7:8]: \"Frank\" → PERSON (0.9986)\n",
      "Span[5:6]: \"Baird\" → PERSON (0.9969)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9878)\n",
      "Span[20:21]: \"Baird’s\" → PERSON (1.0)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9903)\n",
      "Span[5:6]: \"Frank\" → PERSON (0.9981)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9839)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9915)\n",
      "Span[31:32]: \"Highland\" → GPE (0.3642)\n",
      "Span[9:10]: \"Baird’s\" → PERSON (0.998)\n",
      "Span[43:44]: \"Baird\" → PERSON (0.9993)\n",
      "Span[6:7]: \"Let’s\" → PERSON (0.8956)\n",
      "Span[2:3]: \"Frank\" → PERSON (0.9985)\n",
      "Span[15:17]: \"Trembling Jock\" → PERSON (0.6661)\n",
      "Span[18:20]: \"Trembling Jock\" → PERSON (0.7236)\n",
      "Span[49:50]: \"Mountgerald\" → PERSON (0.8975)\n",
      "Span[79:82]: \"the eighteenth century\" → DATE (0.816)\n",
      "Span[76:79]: \"four Old Days\" → DATE (0.5668)\n",
      "Span[38:39]: \"Frank\" → PERSON (0.9945)\n",
      "Span[9:10]: \"Frank’s\" → PERSON (0.9903)\n",
      "Span[3:4]: \"Baird\" → PERSON (0.9946)\n",
      "Span[28:29]: \"Crook\" → PERSON (0.9998)\n",
      "Span[9:10]: \"Randall\" → PERSON (0.9996)\n",
      "Span[1:2]: \"Randall’s\" → PERSON (0.9973)\n",
      "Span[4:5]: \"Crook\" → PERSON (0.9999)\n",
      "Span[13:14]: \"I’ve\" → PERSON (0.7647)\n",
      "Span[6:7]: \"Baird\" → PERSON (0.997)\n",
      "Span[10:11]: \"Randall’s\" → PERSON (0.9999)\n",
      "Span[2:3]: \"Crook’s\" → PERSON (0.9995)\n",
      "Span[3:4]: \"Crook\" → PERSON (0.9999)\n",
      "Span[82:83]: \"Frank\" → PERSON (0.9968)\n",
      "Span[89:90]: \"Inverness\" → GPE (0.9879)\n",
      "Span[3:4]: \"Frank\" → PERSON (0.9991)\n",
      "Span[18:19]: \"Baird\" → PERSON (0.9991)\n",
      "Span[32:33]: \"Frank\" → PERSON (0.9986)\n",
      "Span[3:4]: \"Baird\" → PERSON (0.9924)\n",
      "Span[22:23]: \"Randall\" → PERSON (0.9999)\n",
      "Span[0:2]: \"Aye Fionn\" → PERSON (0.7983)\n",
      "Span[3:7]: \"the Feinn ye ken\" → PERSON (0.7763)\n",
      "Span[2:3]: \"Frank\" → PERSON (0.9982)\n",
      "Span[9:10]: \"Baird\" → PERSON (0.999)\n",
      "Span[16:17]: \"Frank\" → PERSON (0.9846)\n",
      "Span[44:45]: \"It’s\" → GPE (0.3957)\n",
      "Span[52:55]: \"a thousand years\" → DATE (0.7403)\n",
      "Span[6:7]: \"Baird’s\" → PERSON (0.9951)\n",
      "Span[19:20]: \"Frank\" → PERSON (0.9997)\n",
      "Span[29:30]: \"Bainbridge\" → PERSON (0.9999)\n",
      "Span[47:48]: \"Bainbridge\" → PERSON (0.9988)\n",
      "Span[55:57]: \"Perth Harbor\" → GPE (0.488)\n",
      "Span[11:12]: \"Frank\" → PERSON (0.9992)\n",
      "Span[19:20]: \"Bainbridge\" → PERSON (0.9987)\n",
      "Span[10:11]: \"Frank\" → PERSON (0.9957)\n",
      "Span[26:27]: \"Frank\" → PERSON (0.9986)\n",
      "Span[41:42]: \"Bainbridge\" → PERSON (0.9999)\n",
      "Span[12:13]: \"Bainbridge’s\" → PERSON (0.9996)\n",
      "Span[30:31]: \"Bainbridge\" → PERSON (0.9998)\n",
      "Span[15:16]: \"Bainbridge’s\" → PERSON (0.9987)\n",
      "Span[42:43]: \"Bainbridge’s\" → PERSON (0.9955)\n",
      "Span[47:48]: \"Frank\" → PERSON (0.9893)\n",
      "Span[7:8]: \"Bainbridge\" → PERSON (1.0)\n",
      "Span[18:19]: \"Frank’s\" → PERSON (0.9995)\n",
      "Span[40:42]: \"two years\" → DATE (0.7025)\n",
      "Span[59:60]: \"Frank\" → PERSON (0.9977)\n",
      "Span[1:2]: \"Bainbridge\" → PERSON (1.0)\n",
      "Span[4:5]: \"Frank\" → PERSON (0.9997)\n",
      "Span[13:14]: \"Claire\" → PERSON (0.9029)\n",
      "Span[16:19]: \"Gadzooks’ The Gad’\" → PERSON (0.6253)\n",
      "Span[6:7]: \"I’ve\" → PERSON (0.9477)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.993)\n",
      "Span[3:4]: \"Bainbridge\" → PERSON (0.9999)\n",
      "Span[16:17]: \"Williamson\" → PERSON (0.9954)\n",
      "Span[18:20]: \"New York\" → GPE (0.7154)\n",
      "2023-05-14 12:29:46,132 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Span[9:10]: \"Baird\" → PERSON (0.9981)\n",
      "Span[19:20]: \"Frank\" → PERSON (0.9978)\n",
      "Span[5:6]: \"Frank\" → PERSON (0.965)\n",
      "Span[19:20]: \"Bainbridge\" → PERSON (0.9999)\n",
      "Span[55:56]: \"Frank\" → PERSON (0.9905)\n",
      "Span[61:63]: \"L’Heure Bleu\" → PERSON (0.8679)\n",
      "Span[1:2]: \"Baird’s\" → PERSON (0.9978)\n",
      "Span[15:16]: \"Frank\" → PERSON (0.9959)\n",
      "Span[32:34]: \"L’Heure Bleu\" → PERSON (0.8695)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9931)\n",
      "Span[30:31]: \"Frank\" → PERSON (0.995)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9842)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9978)\n",
      "Span[25:26]: \"Frank\" → PERSON (0.9943)\n",
      "Span[14:15]: \"Frank\" → PERSON (0.9974)\n",
      "Span[3:4]: \"Frank\" → PERSON (0.9971)\n",
      "Span[1:2]: \"Frank\" → PERSON (0.8532)\n",
      "Span[67:68]: \"That’s\" → PERSON (0.6427)\n",
      "Span[14:15]: \"Frank\" → PERSON (0.9888)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9964)\n",
      "Span[8:9]: \"Claire\" → PERSON (0.93)\n",
      "Span[18:19]: \"Pembroke\" → GPE (0.4427)\n",
      "Span[15:16]: \"Camerons\" → PERSON (0.9274)\n",
      "Span[21:22]: \"Amiens\" → GPE (0.4893)\n",
      "Span[28:29]: \"Caen\" → GPE (0.4435)\n",
      "Span[35:36]: \"Gordons\" → PERSON (0.793)\n",
      "Span[25:26]: \"He’d\" → PERSON (0.393)\n",
      "Span[72:73]: \"Chisholm\" → PERSON (0.8716)\n",
      "Span[77:78]: \"I’m\" → PERSON (0.8864)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9951)\n",
      "Span[17:18]: \"Don’t\" → PERSON (0.8681)\n",
      "2023-05-14 12:30:30,176 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Span[1:2]: \"Frank’s\" → PERSON (0.7671)\n",
      "Span[38:41]: \"Bonnie Prince Charlie\" → PERSON (0.6268)\n",
      "Span[44:45]: \"Baird\" → PERSON (0.9994)\n",
      "Span[7:8]: \"Claire\" → PERSON (0.9878)\n",
      "Span[10:12]: \"six years\" → DATE (0.8497)\n",
      "Span[37:38]: \"Frank\" → PERSON (0.9397)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.9916)\n",
      "Span[15:16]: \"Lot’s\" → PERSON (0.9587)\n",
      "Span[29:30]: \"Claire\" → PERSON (0.8425)\n",
      "Span[0:1]: \"Frank\" → PERSON (0.991)\n",
      "Span[35:37]: \"six years\" → DATE (0.7775)\n"
     ]
    }
   ],
   "source": [
    "tagger = SequenceTagger.load(\"flair/ner-english-ontonotes-fast\")\n",
    "\n",
    "for paragraph in tchapters[1]:\n",
    "    sentence = Sentence(paragraph)\n",
    "    tagger.predict(sentence)\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        if entity.tag=='PERSON' or entity.tag=='GPE'or entity.tag=='DATE':\n",
    "            print(entity)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-14T12:27:19.360681Z",
     "end_time": "2023-05-14T12:30:47.866133Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
